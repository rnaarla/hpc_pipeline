output_dir: ./runs
enable_profiler: true
enable_training: true
enable_recovery: true
enable_observability: true
enable_benchmarking: true

mode: amp   # options: amp | ddp | deepspeed
epochs: 2
max_steps_per_epoch: 50
batch_size: 32
dataset: s3://mybucket/llm-dataset
profile_steps: 50
model:
  name: gpt-3.5-turbo
  version: v1.0
  parameters:
    hidden_size: 4096
    num_layers: 24
    num_attention_heads: 32
    intermediate_size: 11008
    dropout_rate: 0.1
    max_position_embeddings: 2048
    vocab_size: 50257
    activation_function: gelu
    initializer_range: 0.02
    layer_norm_epsilon: 1e-5
    use_cache: true # whether to use cached attention for faster inference  